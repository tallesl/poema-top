# Poema ğŸ”

Gerando poemas com um modelo de linguagem simples autoregressivo, utilizando Keras e TensorFlow. 

O modelo Ã© uma rede neural recorrente (GRU seguido de softmax + cross-entropy) treinada para gerar o prÃ³ximo caractere
dada uma sequÃªncia de caracteres de entrada.

## Por quÃª poemas?

Poemas geralmente possuem baixa complexidade, pois sÃ£o curtos e repetem temas e padrÃµes rÃ­tmicos. Isso faz com que sua
criaÃ§Ã£o seja menos desafiadora para uma rede neural generativa.

AlÃ©m disso, as pessoas extrapolam o significado de textos poÃ©ticos, isto Ã©, mesmo quando o conteÃºdo Ã© vago elas
encontram maneiras de atribuir sentido a ele.

Outra caracterÃ­stica favorÃ¡vel Ã© a repetiÃ§Ã£o, um efeito muitas vezes indesejÃ¡vel de modelos pequenos (especialmente
utilizando baixas temperaturas) que Ã© amenizado pelo fato de a repetiÃ§Ã£o ser um recurso estilÃ­stico desse gÃªnero
literÃ¡rio.

## Poemas nÃ£o tÃ£o bons

Esse repositÃ³rio Ã© um exercÃ­cio didÃ¡tico, ilustrando como alguns milhares de pontos flutuantes conseguem generalizar
sÃ­labas, palavras, e pequenos versos. Ilustra-se tambÃ©m o quÃ£o pequeno e simples Ã© cÃ³digo (especialmente utilizando a
bibliotecas Keras).

NÃ£o Ã© um objetivo deste modelo gerar poemas de alta qualidade, com recursos linguÃ­sticos avanÃ§ados, mÃ©tricas precisas,
etc.

## Executando a aplicaÃ§Ã£o

A maioria das tarefas de desenvolvimento foram capturadas no arquivo [justfile](src/justfile), as principais sÃ£o:

- `just treina`: inicia o treino de um novo modelo
- `just amostra`: lÃª o Ãºltimo modelo da pasta [modelos\_treinados/temp/](modelos_treinados/temp/) e gera, com diferentes
temperaturas, algumas amostras na linha de comando
- `just infere`: lÃª o Ãºltimo modelo da pasta [modelos\_treinados/temp/](modelos_treinados/temp/) e gera caracteres
continuamente na linha de comando, conforme a temperatura e o texto inicial dado
- `just websocket`: inicia uma aplicaÃ§Ã£o que carrega o modelo, gera caracteres continuamente, e serve-os via websocket

Para utilizar os comandos acima basta instalar o executor de comandos [`just`](https://github.com/casey/just) (
`apt install just` se estiver utilizando Ubuntu).

## Dataset sintÃ©tico

[![](imagens/ollama.png)](#)

Foram criados **datasets com mil, 10 mil, e 100 mil poemas** (ver diretÃ³rio [dataset/](dataset/)). Os **datasets sÃ£o
sintÃ©ticos**, isto Ã©, foram gerados por outro modelo (Llama3.1 8B servido pelo [Ollama](https://ollama.com)).

Exemplo de poema gerado pela LLM:

```
AmanhÃ£ chove
nuvens cinzentas
papÃ©is rasgados
sÃ³ chuva no meu olhar

um vento forte
me sacode a alma
dor de nÃ£o ter
quem me acalme

a chuva cessa
sol de repente
verde queima em mim
amor sem quem amar
```

Gerando texto com Ollama:

```
$ wget http://localhost:11434/api/generate --quiet -O- --post-data="{\"model\": \"llama3.1\", \"prompt\": \"MY PROMPT\", \"stream\": false}" | jq --raw-output ".response"
```

Gerando 100 poemas:

```
$ for i in $(seq 1 100); do echo -ne "\r$i" && echo -e "`./llama3.1 escreva um poema curto em portuguÃªs do Brasil, sem tÃ­tulo, sem enunciado, e com linguajar simples`\n@ >> poemas.txt; done
```

Contando a quantidade de ocorrÃªncias (pelo terminador "@"):

```
$ grep -c "^@$" poemas-10k.txt
10000
$ grep -c "^@$" poemas-100k.txt
100000
```

Removendo linhas que contenham `Aqui vai\*:` e `Note\*:`:

```
$ sed -i '/[Aa]qui vai.*:/,+1d' poemas-100000.txt 
$ sed -i '/[N]ote.*:/d' poemas-100000.txt 
```

## VocabulÃ¡rio

O vocabulÃ¡rio estÃ¡ sendo gerado dinamicamente em tempo de execuÃ§Ã£o, conforme os seguintes passos:

- LÃª-se todo o dataset para memÃ³ria
- Extrai-se uma lista de caracteres Ãºnicos de todo o texto
- Ordena-os alfabeticamente
- Cada caractere passa a ser identificado pelo seu Ã­ndice nessa lista

Os datasets disponibilizados neste repositÃ³rio geram o seguinte vocabulÃ¡rio:

```
!"'(),-./012357:;?abcdefghijklmnopqrstuvwxyzÃ Ã¡Ã¢Ã£Ã§Ã¨Ã©ÃªÃ­Ã³Ã´ÃµÃºÃ¼
```

## Modelo

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Entrada (60, 60)  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           â”‚
           â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Camada GRU #1 (128) â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           â”‚
           â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Camada GRU #2 (128) â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           â”‚
           â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      SaÃ­da (60)     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

A **camada de entrada** tem tamanho 'janela de caracteres anteriores' x 'tamanho do vocabulÃ¡rio'. O tamanho da janela de
caracteres anteriores Ã© um dos hiperparÃ¢metros do modelo (60 caracteres). Cada caractere Ã© codificado em 'one-hot', de
acordo com o vocabulÃ¡rio do modelo, que Ã© gerado dinamicamente no Ã­nicio da aplicaÃ§Ã£o (60 caracteres tambÃ©m,
coincidentemente).

O modelo possui duas **camadas ocultas** com 128 unidades GRU cada. A funÃ§Ã£o de ativaÃ§Ã£o e a funÃ§Ã£o de ativaÃ§Ã£o
recorrente sÃ£o a tangente hiperbÃ³lica e sigmÃ³ide respectivamente ([padrÃ£o do Keras](https://keras.io/api/layers/recurrent_layers/gru/)).

A **camada de saÃ­da** gera um valor de probabilidade para cada caractere do vocabulÃ¡rio, ou seja, tem o mesmo tamanho do
vocabulÃ¡rio. Ã‰ utilizado softmax + cross-entropy como funÃ§Ã£o de ativaÃ§Ã£o.

O modelo possui 179.772 parÃ¢metros no total (pesos + vieses), conforme o seguinte cÃ¡culo
([fonte](https://stats.stackexchange.com/a/328927)):

- Entrada para oculta:
  - m = 60, n = 128
  - 3(nÂ²+nm+2n)
  - 3(128Â² + 7680 + 256)
  - 72960

- Oculta GRU #1 para GRU #2:
  - m = 128, n = 128
  - 3(nÂ²+nm+2n)
  - 3(128Â² + 16384 + 256)
  - 99072

- Oculta GRU #2 para saÃ­da:
  - m = 128, n = 60
  - (128\*60)+60
  - 7740
- Total = 72960 + 99072 + 7740 = 179772

Os mesmos valores podem ser vistos ao chamar [`modelo.summary`](https://keras.io/api/models/model/#summary-method):

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Layer (type)                         â”ƒ Output Shape                â”ƒ         Param # â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ gru (GRU)                            â”‚ (None, 60, 128)             â”‚          72,960 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gru_1 (GRU)                          â”‚ (None, 128)                 â”‚          99,072 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (Dense)                        â”‚ (None, 60)                  â”‚           7,740 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Treino

O modelo utiliza aprendizado auto-supervisionado: o dataset Ã© quebrado em janelas de texto, e para dada janela, o modelo
tenta inferir qual o prÃ³ximo caractere.

A tradicional combinaÃ§Ã£o **softmax + cross-entropy** Ã© utilizada como funÃ§Ã£o de perda. **RMSProp** foi a funÃ§Ã£o de
otimizaÃ§Ã£o escolhida (root mean squared propagation).

O modelo disponiblizado neste repositÃ³rio foi treinado com uma **taxa de aprendizado de 0.001** e ajustado por **115
Ã©pocas** (165 Ã©pocas decorridas, paciÃªncia de 50 Ã©pocas):

[![](modelos_treinados/20240930-123129-loss.png)](#)

Tomou-se 2 horas e 18 minutos para treinar o modelo com o seguinte hardware:

[![](imagens/geforce.png)](#)

- NVIDIA GeForce RTX 2060 6GB (1920 CUDA cores, 192-bit memory bus, resizable BAR habilitado na
BIOS)
- AMD Ryzen 5 5500 (6 cores, 12 threads, 3.6 GHz)
- 32GB RAM (3.2 GHz, XMP/DOCP habilitado na BIOS)

## InferÃªncia

A inferÃªncia Ã© realizada de acordo com os seguintes passos:

- Uma janela de texto 'anterior' de tamanho fixo Ã© passada (seed da geraÃ§Ã£o)
- Cada caractere da janela Ã© codificado em one-hot de acordo com os Ã­ndices do vocabulÃ¡rio
- Um feed forward Ã© feito na rede e uma probabilidade (normalizada entre 0 e 1 pela funÃ§Ã£o softmax) Ã© gerada para cada
caractere do vocabulÃ¡rio
- Para cada probabilidade, aplica-se a funÃ§Ã£o de logaritmo natural e divide-se o valor pela temperatura dada (quanto
maior o valor, diminui-se as diferenÃ§as, mais criativa a resposta)
- Ã‰ obtido um caractere aleatoriamente de acordo com a distribuiÃ§Ã£o multinomial destas probabilidades (com temperatura
aplicada agora)
- ObtÃ©m-se o Ã­ndice no vocabulÃ¡rio do caractere escolhido
- Consulta-se o vocabulÃ¡rio e obtÃ©m-se o caractere a partir do Ã­ndice
- O novo caractere inferido Ã© adicionado a janela de texto 'anterior', permitindo inferÃªncia em loop gerando o texto
continuamente (autorregressÃ£o)

## Exemplos

Exemplo de poema gerado pelo modelo, com **temperatura 0.05** (muito baixa):

```
as estrelas brilham como diamantes
no cÃ©u noturno escuro
um sol amarelo brilha
no cÃ©u azul de manhÃ£
as estrelas brilham como diamantes
no cÃ©u escuro da noite
as estrelas brilham como diamantes
no cÃ©u azul de mar, as estrelas brilham
um luar que me faz sonhar com a paz
```

**Temperatura 0.35** (equilibrado):

```
o vento sussurra segredos
aos meus pensamentos e danÃ§ando
a lua cheia brilha forte
no cÃ©u de goiaba,
o sol brilha forte,
e o silÃªncio Ã© um abraÃ§o
```

**Temperatura 1.20** (muito alta)

```
a chuva desafupadora
um suspiro da atorficÃ£
e eu sinto Ã¡gua fria
apenar aqui estou lÃ¡
de dias de pensa soba,
sua culhbra um vazer
```

## Website poema.top

[![](imagens/website.gif)](#)

O website [https://poema.top](https://poema.top) disponibiliza uma pÃ¡gina que, via websocket, continuamente recebe e
exibe caracteres gerados pelo modelo. Tanto o **domÃ­nio** quanto o **servidor** em que a aplicaÃ§Ã£o estÃ¡ hospedada tÃªm um
custo muito baixo: **$1,61 dolÃ¡res/ano** e **$1,40 dolÃ¡res/mÃªs** respectivamente.

ConfiguraÃ§Ã£o do servidor (VPS):

- Ubuntu Server 22.04
- 1 vCPU (Xeon E5-2660 2.20GHz)
- 1.5 GB RAM

O deploy nÃ£o poderia ser mais simples:

- Alterado o endereÃ§o presente no arquivo `index.html` (presente neste repositÃ³rio) para `wss://poema.top/ws/`
- Nginx (ver diretÃ³rio [nginx/](nginx/))
- Certificado SSL obtido com [Let's Encrypt](https://letsencrypt.org)
- AplicaÃ§Ã£o iniciada a partir de uma sessÃ£o SSH com o comando `nohup` (nada de serviÃ§o systemd e muito menos imagem
Docker)

(site disponibilizado em outubro de 2024, talvez fique fora do ar em um futuro nÃ£o tÃ£o distante!)

## Fontes

[![](imagens/karpathy.png)](#)

- Existem vÃ¡rios exemplos didÃ¡ticos de modelos similares Ã  esse, sendo talvez o mais acessÃ­vel e importante deles a
postagem ["The Unreasonable Effectiveness of Recurrent Neural Networks"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/),
por **Andrej Karpathy**
- Ã‰ importante reconhecer tambÃ©m o trabalho de **TomÃ¡Å¡ Mikolov**, um dos pioneiros na utilizaÃ§Ã£o de redes neurais
recorrentes para modelos de linguagem (["Recurrent neural network based language model](https://www.researchgate.net/profile/Martin-Karafiat/publication/221489926_Recurrent_neural_network_based_language_model/links/0c960523991065d41b000000/Recurrent-neural-network-based-language-model.pdf))
- Ã‰ comum tambÃ©m encontrar menÃ§Ã£o deste tipo de rede em livros sobre aprendizado profundo, segue alguns exemplos (clique
na capa):

[![](imagens/chollet-capa.png)](imagens/chollet.png)

[![](imagens/homl-capa.png)](imagens/homl.png)

[![](imagens/rnn-python-capa.png)](imagens/rnn-python.png)

[![](imagens/scratch-capa.png)](imagens/scratch.png)

## PrÃ³ximos passos

- Avaliar o uso de [LiteRT](https://ai.google.dev/edge/litert) para inferÃªncia

- Treinar o modelo para gerar o poema do inÃ­cio ao fim, isto Ã©, comeÃ§ando com zero caracteres anteriores e gerando um
caractere/token de fim do poema

- Utilizar/criar mÃ©todos de avaliaÃ§Ã£o da qualidade do modelo:
  - Perplexidade (2 elevado Ã  perda mÃ©dia calculada pela funÃ§Ã£o cross-entropy)
  - Gerar um vocabulÃ¡rio de 2-gram caracteres a partir do dataset e calcular percentual de 2-gram invÃ¡lidos em n
caracteres gerados
  - Gerar um vocabulÃ¡rio de palavras a partir do dataset e calcular percentual de palavras invÃ¡lidas em n caracteres
gerados (remover primeira e Ãºltima palavra)
  - Solicitar Ã  uma LLM uma avaliaÃ§Ã£o (nota de 0 a 10 por exemplo) nos quesitos coesÃ£o e coÃªrencia ("LLM-as-a-judge")

- Utilizar o dataset maior (100 mil):
  - Carregar dinamicamente?
  - Continuar carregando totalmente em memÃ³ria, mas quebrar o dataset em datasets menores e rotacionar o dataset em uso
a cada n Ã©pocas?

- Utilizar um vocabulÃ¡rio com tokens de mÃºltiplos caracteres:
  - Gerar um vocabulÃ¡rio, a partir do dataset, seguindo a regra de divisÃ£o silÃ¡bica da lÃ­ngua portuguesa?
  - SerÃ¡ factÃ­vel manter one-hot com esse vocabulÃ¡rio maior ou serÃ¡ necessÃ¡rio treinar/utilizar uma matriz de embedding?
  
- Gerar um dataset sintÃ©tico com todas as probabilidades, de todos os tokens, a cada token inferido pela LLM:
  - Experimentar treinar o modelo calculando a perda contra todas as probabilidades da LLM, e nÃ£o sÃ³ do escolhido (ver
[Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) e
[Dark Knowledge](https://www.ttic.edu/dl/dark14.pdf)
  - Para isso deve-se utilizar o mesmo vocabulÃ¡rio/tokenizador da LLM (o que opÃµe o vocabulÃ¡rio silÃ¡bico proposta mais
acima)
  - ApÃ³s gerado o dataset, para reduzir o tamanho do modelo, pode-se gerar um novo vocabulÃ¡rio que Ã© um subset do
vocabulÃ¡rio da LLM (com apenas os tokens Ãºnicos presentes no dataset)
  - SerÃ¡ factÃ­vel manter one-hot com esse vocabulÃ¡rio maior ou serÃ¡ necessÃ¡rio treinar/utilizar uma matriz de embedding?
